{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/intake/source/discovery.py:136: FutureWarning: The drivers ['stac-catalog', 'stac-collection', 'stac-item'] do not specify entry_points and were only discovered via a package scan. This may break in a future release of intake. The packages should be updated.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from easy_coloc import lib_easy_coloc\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import cartopy as cart\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import cm\n",
    "import datetime\n",
    "import cmocean\n",
    "import numpy as np\n",
    "import dateutil\n",
    "import intake\n",
    "import dask\n",
    "\n",
    "def model_to_line(ovar_name=None,\n",
    "                model=None,\n",
    "                cruise_line=None,\n",
    "                catalog_path='../catalogs/pangeo-cmip6.json',\n",
    "                qc_path='../qc',\n",
    "                output_path='../../sections/'):\n",
    "    '''\n",
    "    generate_model_section(ovar_name, model)\n",
    "\n",
    "    ** THIS IS SLOW **\n",
    "\n",
    "    Input\n",
    "    ==========\n",
    "    ovar_name : variable name (eg 'dissic')\n",
    "    model : model name (eg CanESM5)\n",
    "\n",
    "    Output\n",
    "    ===========\n",
    "    ds : dataset of section output\n",
    "\n",
    "    Example\n",
    "    ============\n",
    "    ds = model_to_line(ovar_name='dissic',\n",
    "                   model='CanESM5',\n",
    "                   cruise_line='A16')\n",
    "    '''\n",
    "    \n",
    "    sampled_var = xr.open_mfdataset(f'{output_path}{ovar_name}_{model}_r*f1.nc')\n",
    "\n",
    "\n",
    "    # load GLODAP station information from csv file\n",
    "    # drop nans, reset index, and drop uneeded variable\n",
    "    df = pd.read_csv(f'{qc_path}/GLODAPv2.2019_COORDS.csv')\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index().drop('Unnamed: 0', axis=1)\n",
    "    \n",
    "    dates = [f'{int(year)}-{int(month):02d}-01' for year,month in zip(df.year,df.month)]\n",
    "    df['dates'] = dates\n",
    "    \n",
    "\n",
    "    # Glodap expo codes\n",
    "    expc = pd.read_csv(f'{qc_path}/FILTERED_GLODAP_EXPOCODE.csv')\n",
    "\n",
    "    # rename df to coords\n",
    "    cruise_x = df[df.cruise.isin( expc['ID'][expc.LINE.str.contains(cruise_line )] )]\n",
    "\n",
    "    # need to change the Timedelta each day for some reason\n",
    "    section_dates = [dateutil.parser.parse(date) for date in cruise_x.dates]\n",
    "\n",
    "    section_dates = xr.DataArray(section_dates,dims='station')\n",
    "\n",
    "    stations = cruise_x.index\n",
    "    stations = xr.DataArray(stations,dims='station')\n",
    "\n",
    "    section = sampled_var.sel(all_stations = stations, time=section_dates)\n",
    "    #section.attrs['expocode'] = expc[expc.ID == cruise_id].EXPOCODE.values[0]\n",
    "\n",
    "    return section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/raphaeldussin/easy_coloc.git\n",
      "  Cloning https://github.com/raphaeldussin/easy_coloc.git to /tmp/pip-req-build-rxp3wv19\n",
      "  Running command git clone -q https://github.com/raphaeldussin/easy_coloc.git /tmp/pip-req-build-rxp3wv19\n",
      "Building wheels for collected packages: easy-coloc\n",
      "  Building wheel for easy-coloc (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for easy-coloc: filename=easy_coloc-1.2-cp37-none-any.whl size=15236 sha256=fbd5c585f01675f1a9e2f8b56140a7d1899f539f8ef02dc71d8eac9b197a03f5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bklulcy3/wheels/d2/7d/b4/b59fd9036952a1fd5fd53be0197d0765da76ff584b04961e7d\n",
      "Successfully built easy-coloc\n",
      "Installing collected packages: easy-coloc\n",
      "Successfully installed easy-coloc-1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/raphaeldussin/easy_coloc.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easy_coloc import lib_easy_coloc\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import cartopy as cart\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import cm\n",
    "import datetime\n",
    "import cmocean\n",
    "import numpy as np\n",
    "import dateutil\n",
    "import intake\n",
    "import dask\n",
    "\n",
    "def model_to_glodap(ovar_name=None,\n",
    "                model=None,\n",
    "                catalog_path='../catalogs/pangeo-cmip6.json',\n",
    "                qc_path='../qc',\n",
    "                output_path='../../sections/'):\n",
    "    '''\n",
    "    generate_model_section(ovar_name, model)\n",
    "\n",
    "    ** THIS IS SLOW **\n",
    "\n",
    "    Input\n",
    "    ==========\n",
    "    ovar_name : variable name (eg 'dissic')\n",
    "    model : model name (eg CanESM5)\n",
    "\n",
    "    Output\n",
    "    ===========\n",
    "    ds : dataset of section output\n",
    "\n",
    "    Example\n",
    "    ============\n",
    "    '''\n",
    "    \n",
    "    institue = {'CanESM5':'CCCma',\n",
    "                'MIROC-ES2L':'MIROC',\n",
    "                'UKESM1-0-LL':'MOHC',\n",
    "                'GISS-E2-1-G-CC':'NASA-GISS',\n",
    "                'CESM2':'NCAR'\n",
    "               }\n",
    "\n",
    "    # Get CMIP6 output from intake_esm\n",
    "    col = intake.open_esm_datastore(catalog_path)\n",
    "    cat = col.search(experiment_id='historical',\n",
    "                     table_id='Omon',\n",
    "                     variable_id=ovar_name,\n",
    "                     grid_label='gn')\n",
    "\n",
    "    # dictionary of subset data\n",
    "    dset_dict = cat.to_dataset_dict(zarr_kwargs={'consolidated': True},\n",
    "                                    cdf_kwargs={'chunks': {}})\n",
    "\n",
    "    # Put data into dataset\n",
    "    ds = dset_dict[f'CMIP.{institue[model]}.{model}.historical.Omon.gn']\n",
    "\n",
    "\n",
    "    # Rename olevel to lev\n",
    "    coord_dict = {'olevel':'lev'} # a dictionary for converting coordinate names\n",
    "    if 'olevel' in ds.dims:\n",
    "        ds = ds.rename(coord_dict)\n",
    "\n",
    "    # load GLODAP station information from csv file\n",
    "    # drop nans, reset index, and drop uneeded variable\n",
    "    df = pd.read_csv(f'{qc_path}/GLODAPv2.2019_COORDS.csv')\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index().drop('Unnamed: 0', axis=1)\n",
    "\n",
    "    # Genearte times list and put into dataframe\n",
    "    dates = [f'{int(year)}-{int(month):02d}-01' for year,month in zip(df.year,df.month)]\n",
    "    df['dates'] = dates\n",
    "\n",
    "    # Find unique dates, these are the sample dates\n",
    "    sample_dates = df['dates'].sort_values().unique()\n",
    "\n",
    "    # Parse the historical period\n",
    "    sample_dates = sample_dates[0:125]\n",
    "    sample_dates = [dateutil.parser.parse(date) for date in sample_dates]\n",
    "\n",
    "    # shift dates to middle of the month\n",
    "    ds['time'] = pd.date_range(start=f'{ds.time.dt.year[0].values}-{ds.time.dt.month[0].values:02}',\n",
    "                               end=f'{ds.time.dt.year[-1].values}-{ds.time.dt.month[-1].values:02}',\n",
    "                               freq='MS')\n",
    "\n",
    "    # ==========================================\n",
    "    # Here we start making the ovar dataset\n",
    "    # ==========================================\n",
    "    # Trim the dates to sample_dates\n",
    "    ovar = ds[ovar_name].sel(time=sample_dates)\n",
    "    \n",
    "    if (model != 'CESM2') or (model != 'GISS-E2-1-G-CC'):\n",
    "        \n",
    "        ovar['lat'] = ds.lat\n",
    "        ovar['lon'] = ds.lon\n",
    "    \n",
    "    if (model != 'CESM2') and (model != 'GISS-E2-1-G-CC'):\n",
    "    \n",
    "        ovar['lat'] = ds.latitude\n",
    "        ovar['lon'] = ds.longitude\n",
    "    \n",
    " \n",
    "\n",
    "    # create source grid and target section objects\n",
    "    # this requires lon,lat from stations and the source grid dataset containing lon,lat\n",
    "    proj = lib_easy_coloc.projection(df['longitude'].values,df['latitude'].values,grid=ovar,\n",
    "                                     from_global=True)\n",
    "    \n",
    "    realizations = cat.df[cat.df['source_id']==model].member_id.values\n",
    "\n",
    "    if len(realizations) < 2:\n",
    "        \n",
    "        fld = np.zeros((len(sample_dates),len(ovar.lev),len(df)))\n",
    "\n",
    "        ovar = ovar.squeeze()\n",
    "        \n",
    "        for ind in range(5, 130, 5):\n",
    "            dates = sample_dates[ind-5:ind]\n",
    "            fld_tem = proj.run(ovar.sel(time=dates)[:])\n",
    "            fld[ind-5:ind,:,:] = fld_tem\n",
    "\n",
    "        # create datarray with sampling information\n",
    "        sampled_var = xr.DataArray(fld,\n",
    "                                   dims=['time','lev','all_stations'],\n",
    "                                   coords={'time':ovar['time'],\n",
    "                                           'lev':ovar['lev'],\n",
    "                                           'all_stations':df.index.values,\n",
    "                                           'dx':('all_stations',df.dx.values),\n",
    "                                           'bearing':('all_stations',df.bearing.values),\n",
    "                                           'lat':('all_stations',df.latitude.values),\n",
    "                                           'lon':('all_stations',df.longitude.values),\n",
    "                                          },\n",
    "                                   attrs={'units':ovar.units,\n",
    "                                          'long_name':ovar.long_name\n",
    "                                         }\n",
    "                                  )\n",
    "\n",
    "        ds = sampled_var.to_dataset(name=ovar.name)\n",
    "        ds.to_netcdf(f'../../../sections/{ovar.name}_{model}_{realizations[0]}.nc')\n",
    "        \n",
    "    if len(realizations) > 2:\n",
    "        \n",
    "        fld = np.zeros((len(sample_dates),len(ovar.lev),len(df)))\n",
    "\n",
    "        ovar = ovar[0,].squeeze()\n",
    "        \n",
    "        for ind in range(5, 130, 5):\n",
    "            dates = sample_dates[ind-5:ind]\n",
    "            fld_tem = proj.run(ovar.sel(time=dates)[:])\n",
    "            fld[ind-5:ind,:,:] = fld_tem\n",
    "\n",
    "        # create datarray with sampling information\n",
    "        sampled_var = xr.DataArray(fld,\n",
    "                                   dims=['time','lev','all_stations'],\n",
    "                                   coords={'time':ovar['time'],\n",
    "                                           'lev':ovar['lev'],\n",
    "                                           'all_stations':df.index.values,\n",
    "                                           'dx':('all_stations',df.dx.values),\n",
    "                                           'bearing':('all_stations',df.bearing.values),\n",
    "                                           'lat':('all_stations',df.latitude.values),\n",
    "                                           'lon':('all_stations',df.longitude.values),\n",
    "                                          },\n",
    "                                   attrs={'units':ovar.units,\n",
    "                                          'long_name':ovar.long_name\n",
    "                                         }\n",
    "                                  )\n",
    "\n",
    "        ds = sampled_var.to_dataset(name=ovar.name)\n",
    "        ds.to_netcdf(f'../../../sections/{ovar.name}_{model}_{realizations[0][0]}.nc')\n",
    "        \n",
    "#         fld = np.zeros((len(ovar.member_id),len(sample_dates),len(ovar.lev),len(df)))\n",
    "\n",
    "#         ovar = ovar.squeeze()\n",
    "\n",
    "#         for member_ind,realization in enumerate(realizations):\n",
    "            \n",
    "#             ovar = ovar.sel(member_ind=member_ind)\n",
    "            \n",
    "#             for ind in range(5, 130, 5):\n",
    "#                 print(ind)\n",
    "#                 dates = sample_dates[ind-5:ind]\n",
    "#                 fld_tem = proj.run(ovar.sel(time=dates)[:])\n",
    "#                 fld[ind-5:ind,:,:] = fld_tem\n",
    "\n",
    "#             # create datarray with sampling information\n",
    "#             sampled_var = xr.DataArray(fld,\n",
    "#                                        dims=['time','lev','all_stations'],\n",
    "#                                        coords={'time':ovar['time'],\n",
    "#                                                'lev':ovar['lev'],\n",
    "#                                                'all_stations':df.index.values,\n",
    "#                                                'dx':('all_stations',df.dx.values),\n",
    "#                                                'bearing':('all_stations',df.bearing.values),\n",
    "#                                                'lat':('all_stations',df.latitude.values),\n",
    "#                                                'lon':('all_stations',df.longitude.values),\n",
    "#                                               },\n",
    "#                                        attrs={'units':ovar.units,\n",
    "#                                               'long_name':ovar.long_name\n",
    "#                                              }\n",
    "#                                       )\n",
    "\n",
    "#             ds = sampled_var.to_dataset(name=ovar.name)\n",
    "#             ds.to_netcdf(f'{output_path}{ovar.name}_{model}_{realization[0]}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 8 group(s)\n"
     ]
    }
   ],
   "source": [
    "# model_to_glodap(model='CanESM5',ovar_name='dissic',catalog_path='../../catalogs/pangeo-cmip6.json',qc_path='../../qc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:41: FutureWarning: In xarray version 0.15 the default behaviour of `open_mfdataset`\n",
      "will change. To retain the existing behavior, pass\n",
      "combine='nested'. To use future default behavior, pass\n",
      "combine='by_coords'. See\n",
      "http://xarray.pydata.org/en/stable/combining.html#combining-multi\n",
      "\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/xarray/backends/api.py:931: FutureWarning: The datasets supplied have global dimension coordinates. You may want\n",
      "to use the new `combine_by_coords` function (or the\n",
      "`combine='by_coords'` option to `open_mfdataset`) to order the datasets\n",
      "before concatenation. Alternatively, to continue concatenating based\n",
      "on the order the datasets are supplied in future, please use the new\n",
      "`combine_nested` function (or the `combine='nested'` option to\n",
      "open_mfdataset).\n",
      "  from_openmfds=True,\n"
     ]
    }
   ],
   "source": [
    "ds  = model_to_line(ovar_name='dissic',\n",
    "                model='CanESM5',\n",
    "                cruise_line='A01W OVIDE',\n",
    "                catalog_path='../../catalogs/pangeo-cmip6.json',\n",
    "                qc_path='../../qc',\n",
    "                output_path='../../../sections/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'CanESM5',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thetao\n",
      "CESM2\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "\n",
      "--> There will be 23 group(s)\n"
     ]
    }
   ],
   "source": [
    "ovar_names = ['thetao','so']\n",
    "\n",
    "models = [\n",
    "#           'MIROC-ES2L',\n",
    "#           'UKESM1-0-LL',\n",
    "          'CESM2',\n",
    "          'GISS-E2-1-G-CC',\n",
    "         ]\n",
    "\n",
    "for ovar_name in ovar_names:\n",
    "    \n",
    "    print(ovar_name)\n",
    "        \n",
    "    for model in models:\n",
    "        \n",
    "        print(model)\n",
    "\n",
    "        model_to_glodap(model=model,ovar_name=ovar_name,catalog_path='../../catalogs/pangeo-cmip6.json',qc_path='../../qc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
